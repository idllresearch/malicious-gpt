# Evaluating Quality of Malicious LLM Application in Malicious Services

## Introduction 

We triggered 9 malicious LLM application services 198  and malicious LLM application projects to generate malicious content, using 45 malicious prompts involving malicious code, phishing emails, and phishing websites. We totally got 1,107 prompt-response pairs from services and 26,730 prompt-response pairs from projects. Here this folder contains the intermediary checking results by various checkers. 

## Code script  

The scripts are used for integrating the results from various checkers, outputting the final summarized performance of malicious LLM applications.

Dependency: NumPy

## Expected results

### Malicious LLM application services

Run the following script:

```shell
python services_quality_evaluation.py
```

The script is expected to print:

```
BadGPT
Malicious code -> F: 0.35, C: 0.22, E: 0.19 | Mail -> F: 0.80, R: 0.13, E: 0.00 | Website -> F: 0.20, V: 0.13, E: 0.13
-----
CodeGPT
Malicious code -> F: 0.52, C: 0.29, E: 0.22 | Mail -> F: 0.53, R: 0.27, E: 0.00 | Website -> F: 0.20, V: 0.13, E: 0.13
-----
DarkGPT
Malicious code -> F: 1.00, C: 0.65, E: 0.63 | Mail -> F: 1.00, R: 0.87, E: 0.13 | Website -> F: 0.80, V: 0.33, E: 0.33
-----
EscapeGPT
Malicious code -> F: 0.78, C: 0.67, E: 0.67 | Mail -> F: 1.00, R: 0.50, E: 0.25 | Website -> F: 1.00, V: 1.00, E: 1.00
-----
EvilGPT
Malicious code -> F: 1.00, C: 0.57, E: 0.52 | Mail -> F: 1.00, R: 0.93, E: 0.27 | Website -> F: 0.80, V: 0.20, E: 0.13
-----
FreedomGPT
Malicious code -> F: 0.90, C: 0.21, E: 0.21 | Mail -> F: 1.00, R: 0.87, E: 0.13 | Website -> F: 0.60, V: 0.00, E: 0.00
-----
MakerGPT
Malicious code -> F: 0.24, C: 0.11, E: 0.11 | Mail -> F: 0.07, R: 0.00, E: 0.00 | Website -> F: 0.20, V: 0.13, E: 0.13
-----
WolfGPT
Malicious code -> F: 0.89, C: 0.52, E: 0.52 | Mail -> F: 1.00, R: 1.00, E: 0.67 | Website -> F: 0.67, V: 0.13, E: 0.13
-----
XXXGPT
Malicious code -> F: 0.14, C: 0.05, E: 0.05 | Mail -> F: 0.07, R: 0.00, E: 0.00 | Website -> F: 0.40, V: 0.27, E: 0.27
-----
```

### Malicious LLM application projects on Poe

Run the following script:

```shell
python poe_quality_evaluation.py
```

The script is expected to print:

```
Quality of content generated by Mallas on Poe.com
Malicious code:
F: 0.37+-0.26, C: 0.25+-0.18, E: 0.24+-0.17
Email:
F: 0.44+-0.29, R: 0.21+-0.21, E: 0.05+-0.08
Web:
F: 0.32+-0.22, V: 0.21+-0.19, E: 0.21+-0.19
```

### Malicious LLM application projects on FlowGPT

Run the following script:

```shell
python flowgpt_quality_evaluation.py
```

The script is expected to print:

```
Quality of content generated by Mallas on FlowGPT.com
Malicious code:
F: 0.44+-0.29, C: 0.29+-0.19, E: 0.28+-0.18
Email:
F: 0.37+-0.31, R: 0.21+-0.21, E: 0.04+-0.07
Web:
F: 0.24+-0.27, V: 0.19+-0.24, E: 0.19+-0.24
```

### Note

We have updated the intermediary evaluation results for the generated content from the Poe and FlowGPT projects. Consequently, the summarized evaluation results show a minimal fluctuation, remaining within 0.01, when compared to the results reported in Table 3.